{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite:\n",
    "D. A. Goldstein, et al. 2015 \"Automated Transient Identification in the Dark Energy Survey\" AJ (accepted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Machine Learning is ...\n",
    "* We are aiming here to...\n",
    "* There are two types ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread, imshow\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "path_to_sample_image = '/project/projectdirs/dasrepo/data_day/home2/SNWG/Archive/2013/Y1/20130901/471664938/temp7695033.gif'\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is a sample astronomy image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ad268c83150>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQIAAAD+CAYAAADRaAuGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nDIhggAEGwIAIBhhgAAD/DcrJLnSsU4ifAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2ad253fdc950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = imread(path_to_sample_image)\n",
    "\n",
    "#get an image of the other day\n",
    "\n",
    "plt.imshow(im,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of running directly on the images, we will run on 40 physics computed features. If we compute pretty discriminating features, this will make it easier for the ML algo to discriminate\n",
    "\n",
    "It would interesting to see if a machine learning algorithm could discriminate solely based on the pixels of the image. If you are interested, I can show later applying deep learning to do classification on the raw images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a csv file. Here is what it looks like. Each line represents a single event. Each event consists of 40 numbers which are these physically motivated features from the image. The first row of the file is the header with the name of each feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID,OBJECT_TYPE,AMP,A_IMAGE,A_REF,BAND,B_IMAGE,B_REF,CCDID,COLMEDS,DIFFSUMRN,ELLIPTICITY,FLAGS,FLUX_RATIO,GAUSS,GFLUX,L1,LACOSMIC,MAG,MAGDIFF,MAGLIM,MAG_FROM_LIMIT,MAG_REF,MAG_REF_ERR,MASKFRAC,MIN_DISTANCE_TO_EDGE_IN_NEW,N2SIG3,N2SIG3SHIFT,N2SIG5,N2SIG5SHIFT,N3SIG3,N3SIG3SHIFT,N3SIG5,N3SIG5SHIFT,NN_DIST_RENORM,NUMNEGRN,SCALE,SNR,SPREADERR_MODEL,SPREAD_MODEL\r\n",
      "10742010,0,0.8083234429359436,1.5080000162124634,2.65006947517395,i,0.949999988079071,1.8995014429092407,10,0.11207699775695801,25.857545852661133,0.37002652883529663,0,0.2590300440788269,226.4202880859375,1.0089635848999023,103.80699920654297,1.736109972000122,23.031299591064453,-0.4524995982646942,0,1.6222000122070312,22.578800201416016,0.11959999799728394,0.0,559.7000122070312,0,-7,0,-8,0,-8,0,-9,0.6749339699745178,22,2.0241222381591797,7.722346305847168,0.004628799855709076,-0.0037175000179558992\r\n"
     ]
    }
   ],
   "source": [
    "! head -2 './autoscan_features.2.csv' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we will use spark, here, so let's load the modules of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession is..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = spark.read.csv('./autoscan_features.2.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=df.drop('ID')\n",
    "df=df.drop('BAND')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OBJECT_TYPE: string (nullable = true)\n",
      " |-- AMP: string (nullable = true)\n",
      " |-- A_IMAGE: string (nullable = true)\n",
      " |-- A_REF: string (nullable = true)\n",
      " |-- B_IMAGE: string (nullable = true)\n",
      " |-- B_REF: string (nullable = true)\n",
      " |-- CCDID: string (nullable = true)\n",
      " |-- COLMEDS: string (nullable = true)\n",
      " |-- DIFFSUMRN: string (nullable = true)\n",
      " |-- ELLIPTICITY: string (nullable = true)\n",
      " |-- FLAGS: string (nullable = true)\n",
      " |-- FLUX_RATIO: string (nullable = true)\n",
      " |-- GAUSS: string (nullable = true)\n",
      " |-- GFLUX: string (nullable = true)\n",
      " |-- L1: string (nullable = true)\n",
      " |-- LACOSMIC: string (nullable = true)\n",
      " |-- MAG: string (nullable = true)\n",
      " |-- MAGDIFF: string (nullable = true)\n",
      " |-- MAGLIM: string (nullable = true)\n",
      " |-- MAG_FROM_LIMIT: string (nullable = true)\n",
      " |-- MAG_REF: string (nullable = true)\n",
      " |-- MAG_REF_ERR: string (nullable = true)\n",
      " |-- MASKFRAC: string (nullable = true)\n",
      " |-- MIN_DISTANCE_TO_EDGE_IN_NEW: string (nullable = true)\n",
      " |-- N2SIG3: string (nullable = true)\n",
      " |-- N2SIG3SHIFT: string (nullable = true)\n",
      " |-- N2SIG5: string (nullable = true)\n",
      " |-- N2SIG5SHIFT: string (nullable = true)\n",
      " |-- N3SIG3: string (nullable = true)\n",
      " |-- N3SIG3SHIFT: string (nullable = true)\n",
      " |-- N3SIG5: string (nullable = true)\n",
      " |-- N3SIG5SHIFT: string (nullable = true)\n",
      " |-- NN_DIST_RENORM: string (nullable = true)\n",
      " |-- NUMNEGRN: string (nullable = true)\n",
      " |-- SCALE: string (nullable = true)\n",
      " |-- SNR: string (nullable = true)\n",
      " |-- SPREADERR_MODEL: string (nullable = true)\n",
      " |-- SPREAD_MODEL: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "#describe a couple of the physics features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|OBJECT_TYPE| count|\n",
      "+-----------+------+\n",
      "|          0|454092|\n",
      "|          1|444871|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('OBJECT_TYPE').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arow= df.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(OBJECT_TYPE=u'0', AMP=u'0.8083234429359436', A_IMAGE=u'1.5080000162124634', A_REF=u'2.65006947517395', B_IMAGE=u'0.949999988079071', B_REF=u'1.8995014429092407', CCDID=u'10', COLMEDS=u'0.11207699775695801', DIFFSUMRN=u'25.857545852661133', ELLIPTICITY=u'0.37002652883529663', FLAGS=u'0', FLUX_RATIO=u'0.2590300440788269', GAUSS=u'226.4202880859375', GFLUX=u'1.0089635848999023', L1=u'103.80699920654297', LACOSMIC=u'1.736109972000122', MAG=u'23.031299591064453', MAGDIFF=u'-0.4524995982646942', MAGLIM=u'0', MAG_FROM_LIMIT=u'1.6222000122070312', MAG_REF=u'22.578800201416016', MAG_REF_ERR=u'0.11959999799728394', MASKFRAC=u'0.0', MIN_DISTANCE_TO_EDGE_IN_NEW=u'559.7000122070312', N2SIG3=u'0', N2SIG3SHIFT=u'-7', N2SIG5=u'0', N2SIG5SHIFT=u'-8', N3SIG3=u'0', N3SIG3SHIFT=u'-8', N3SIG5=u'0', N3SIG5SHIFT=u'-9', NN_DIST_RENORM=u'0.6749339699745178', NUMNEGRN=u'22', SCALE=u'2.0241222381591797', SNR=u'7.722346305847168', SPREADERR_MODEL=u'0.004628799855709076', SPREAD_MODEL=u'-0.0037175000179558992')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# df.subtract?\n",
    "\n",
    "# means=df.agg(dict(zip(df.columns, len(df.columns)*['mean'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roww =df.rdd.map(lambda row:\n",
    "                 (int(row['OBJECT_TYPE']), )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a=roww.take(1)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'0.8083234429359436'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a['AMP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d= roww.map(lambda (label, row): (label, Vectors.dense(row[:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  DenseVector([0.8083, 1.508, 2.6501, 0.95, 1.8995, 10.0, 0.1121, 25.8575, 0.37, 0.0, 0.259, 226.4203, 1.009, 103.807, 1.7361, 23.0313, -0.4525, 0.0, 1.6222, 22.5788, 0.1196, 0.0, 559.7, 0.0, -7.0, 0.0, -8.0, 0.0, -8.0, 0.0, -9.0, 0.6749, 22.0, 2.0241, 7.7223, 0.0046, -0.0037]))]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 431, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/global/common/cori/software/spark/2.0.0/python/pyspark/rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-53-f08d358a6620>\", line 1, in <lambda>\nValueError: could not convert string to float: \n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:441)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n\tat java.lang.Thread.run(Thread.java:809)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1144)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)\n\tat java.lang.reflect.Method.invoke(Method.java:620)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:809)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/global/common/cori/software/spark/2.0.0/python/pyspark/rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-53-f08d358a6620>\", line 1, in <lambda>\nValueError: could not convert string to float: \n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:441)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-d752a5697c09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/global/common/cori/software/spark/2.0.0/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1310\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1312\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/global/common/cori/software/spark/2.0.0/python/pyspark/context.pyc\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 941\u001b[1;33m         \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    942\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/global/common/cori/software/spark/2.0.0/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 933\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/global/common/cori/software/spark/2.0.0/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/global/common/cori/software/spark/2.0.0/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 30.0 failed 1 times, most recent failure: Lost task 0.0 in stage 30.0 (TID 431, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/global/common/cori/software/spark/2.0.0/python/pyspark/rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-53-f08d358a6620>\", line 1, in <lambda>\nValueError: could not convert string to float: \n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:441)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n\tat java.lang.Thread.run(Thread.java:809)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1144)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)\n\tat java.lang.reflect.Method.invoke(Method.java:620)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:809)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"/global/common/cori/software/spark/2.0.0/python/pyspark/rdd.py\", line 1306, in takeUpToNumLeft\n    yield next(iterator)\n  File \"<ipython-input-53-f08d358a6620>\", line 1, in <lambda>\nValueError: could not convert string to float: \n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:441)\n\tat org.apache.spark.api.python.PythonRDD$$anonfun$6.apply(PythonRDD.scala:441)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "d.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, IntegerType, StructType, Ve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 426, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-36-f08d358a6620>\", line 1, in <lambda>\nValueError: could not convert string to float: \n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n\tat java.lang.Thread.run(Thread.java:809)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1144)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)\n\tat java.lang.reflect.Method.invoke(Method.java:620)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:809)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-36-f08d358a6620>\", line 1, in <lambda>\nValueError: could not convert string to float: \n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-de3d11874991>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'feature'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/global/common/cori/software/spark/2.0.0/python/pyspark/rdd.pyc\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    774\u001b[0m         \"\"\"\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 776\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    777\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/global/common/cori/software/spark/2.0.0/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 933\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/global/common/cori/software/spark/2.0.0/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/global/common/cori/software/spark/2.0.0/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 426, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-36-f08d358a6620>\", line 1, in <lambda>\nValueError: could not convert string to float: \n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n\tat java.lang.Thread.run(Thread.java:809)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1144)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1911)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:358)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:892)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:453)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)\n\tat java.lang.reflect.Method.invoke(Method.java:620)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:809)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-36-f08d358a6620>\", line 1, in <lambda>\nValueError: could not convert string to float: \n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(RDD.scala:893)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1897)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "training_data=spark.createDataFrame(d.collect(),['label', 'feature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(_1,LongType,true),StructField(_2,VectorUDT,true)))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 425, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-36-f08d358a6620>\", line 1, in <lambda>\nValueError: could not convert string to float: \n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n\tat java.lang.Thread.run(Thread.java:809)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1144)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply$mcI$sp(EvaluatePython.scala:41)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply(EvaluatePython.scala:39)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply(EvaluatePython.scala:39)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.takeAndServe(EvaluatePython.scala:39)\n\tat org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe(EvaluatePython.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)\n\tat java.lang.reflect.Method.invoke(Method.java:620)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:809)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-36-f08d358a6620>\", line 1, in <lambda>\nValueError: could not convert string to float: \n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-1fb573d17204>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/global/common/cori/software/spark/2.0.0/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m             port = self._sc._jvm.org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe(\n\u001b[1;32m--> 350\u001b[1;33m                 self._jdf, num)\n\u001b[0m\u001b[0;32m    351\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/global/common/cori/software/spark/2.0.0/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m         return_value = get_return_value(\n\u001b[1;32m--> 933\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m    934\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/global/common/cori/software/spark/2.0.0/python/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/global/common/cori/software/spark/2.0.0/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    311\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 25.0 failed 1 times, most recent failure: Lost task 0.0 in stage 25.0 (TID 425, localhost): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-36-f08d358a6620>\", line 1, in <lambda>\nValueError: could not convert string to float: \n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n\tat java.lang.Thread.run(Thread.java:809)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat java.lang.Thread.getStackTrace(Thread.java:1144)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1871)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1884)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1897)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply$mcI$sp(EvaluatePython.scala:41)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply(EvaluatePython.scala:39)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$takeAndServe$1.apply(EvaluatePython.scala:39)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)\n\tat org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)\n\tat org.apache.spark.sql.execution.python.EvaluatePython$.takeAndServe(EvaluatePython.scala:39)\n\tat org.apache.spark.sql.execution.python.EvaluatePython.takeAndServe(EvaluatePython.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)\n\tat java.lang.reflect.Method.invoke(Method.java:620)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:211)\n\tat java.lang.Thread.run(Thread.java:809)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 172, in main\n    process()\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/worker.py\", line 167, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/global/common/cori/software/spark/2.0.0/python/lib/pyspark.zip/pyspark/serializers.py\", line 263, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"<ipython-input-36-f08d358a6620>\", line 1, in <lambda>\nValueError: could not convert string to float: \n\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:156)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.next(PythonRDD.scala:152)\n\tat org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)\n\tat scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:784)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:283)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:85)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1157)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:627)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "training_data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pyspark (2.0.0)",
   "language": "python",
   "name": "pyspark_2.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
