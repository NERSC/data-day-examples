{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite:\n",
    "D. A. Goldstein, et al. 2015 \"Automated Transient Identification in the Dark Energy Survey\" AJ (accepted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We are aiming here to classify two different types of astronomy images: true data, and artificially injected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's get the pyspark kernel. Open up a Cori terminal and type \"module load spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab the data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-08-23 06:46:24--  http://portal.nersc.gov/project/dessn/autoscan/autoscan_features.2.csv\n",
      "Resolving portal.nersc.gov... 128.55.6.160\n",
      "Connecting to portal.nersc.gov|128.55.6.160|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 448893905 (428M) [text/plain]\n",
      "Saving to: “autoscan_features.2.csv”\n",
      "\n",
      "100%[======================================>] 448,893,905  106M/s   in 4.0s    \n",
      "\n",
      "2016-08-23 06:46:28 (106 MB/s) - “autoscan_features.2.csv” saved [448893905/448893905]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! rm -rf autoscan_features.2.csv &&  wget http://portal.nersc.gov/project/dessn/autoscan/autoscan_features.2.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread, imshow\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "path_to_sample_image = \"/project/projectdirs/dasrepo/data_day/astron-images/srch11802308.gif\"\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is a sample astronomy image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#im = imread(path_to_sample_image)\n",
    "\n",
    "#get an image of the other day\n",
    "\n",
    "#plt.imshow(im,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of running directly on the images, we will run on 40 physics computed features. If we compute pretty discriminating features, this will make it easier for the ML algo to discriminate\n",
    "\n",
    "It would interesting to see if a machine learning algorithm could discriminate solely based on the pixels of the image. If you are interested, I can show later applying deep learning to do classification on the raw images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a csv file. Here is what it looks like. Each line represents a single event. Each event consists of 40 numbers which are these physically motivated features from the image. The first row of the file is the header with the name of each feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# autoscan training data\r\n",
      "# use the id column to cross-match rows with thumbnails\r\n",
      "# object_type gives the class of the row\r\n",
      "# object_type = 0: artifact\r\n",
      "# object_type = 1: non-artifact\r\n",
      "# remaining 38 columns defined in section 3 and table 2 of companion paper \r\n"
     ]
    }
   ],
   "source": [
    "! head -12 './autoscan_features.2.csv' | grep \"^#\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! sed -i.bak '/^#/d' ./autoscan_features.2.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we will use spark, here, so let's load the modules of interest and delete the comments at the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkSession is like the workhorse variable here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will read the csv file to a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 ms, sys: 2 ms, total: 7 ms\n",
      "Wall time: 8.12 s\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('./autoscan_features.2.csv', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ID will not be useful and band is non-numerical\n",
    "df=df.drop('ID')\n",
    "df=df.drop('BAND')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a sample record from the dataset. As we can see, underneath the dataframe is an RDD of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5 ms, sys: 4 ms, total: 9 ms\n",
      "Wall time: 1.01 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(OBJECT_TYPE=u'0', AMP=u'0.8083234429359436', A_IMAGE=u'1.5080000162124634', A_REF=u'2.65006947517395', B_IMAGE=u'0.949999988079071', B_REF=u'1.8995014429092407', CCDID=u'10', COLMEDS=u'0.11207699775695801', DIFFSUMRN=u'25.857545852661133', ELLIPTICITY=u'0.37002652883529663', FLAGS=u'0', FLUX_RATIO=u'0.2590300440788269', GAUSS=u'226.4202880859375', GFLUX=u'1.0089635848999023', L1=u'103.80699920654297', LACOSMIC=u'1.736109972000122', MAG=u'23.031299591064453', MAGDIFF=u'-0.4524995982646942', MAGLIM=u'0', MAG_FROM_LIMIT=u'1.6222000122070312', MAG_REF=u'22.578800201416016', MAG_REF_ERR=u'0.11959999799728394', MASKFRAC=u'0.0', MIN_DISTANCE_TO_EDGE_IN_NEW=u'559.7000122070312', N2SIG3=u'0', N2SIG3SHIFT=u'-7', N2SIG5=u'0', N2SIG5SHIFT=u'-8', N3SIG3=u'0', N3SIG3SHIFT=u'-8', N3SIG5=u'0', N3SIG5SHIFT=u'-9', NN_DIST_RENORM=u'0.6749339699745178', NUMNEGRN=u'22', SCALE=u'2.0241222381591797', SNR=u'7.722346305847168', SPREADERR_MODEL=u'0.004628799855709076', SPREAD_MODEL=u'-0.0037175000179558992')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the schema. As we can see here, there is one label, one ID and 38 other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OBJECT_TYPE: string (nullable = true)\n",
      " |-- AMP: string (nullable = true)\n",
      " |-- A_IMAGE: string (nullable = true)\n",
      " |-- A_REF: string (nullable = true)\n",
      " |-- B_IMAGE: string (nullable = true)\n",
      " |-- B_REF: string (nullable = true)\n",
      " |-- CCDID: string (nullable = true)\n",
      " |-- COLMEDS: string (nullable = true)\n",
      " |-- DIFFSUMRN: string (nullable = true)\n",
      " |-- ELLIPTICITY: string (nullable = true)\n",
      " |-- FLAGS: string (nullable = true)\n",
      " |-- FLUX_RATIO: string (nullable = true)\n",
      " |-- GAUSS: string (nullable = true)\n",
      " |-- GFLUX: string (nullable = true)\n",
      " |-- L1: string (nullable = true)\n",
      " |-- LACOSMIC: string (nullable = true)\n",
      " |-- MAG: string (nullable = true)\n",
      " |-- MAGDIFF: string (nullable = true)\n",
      " |-- MAGLIM: string (nullable = true)\n",
      " |-- MAG_FROM_LIMIT: string (nullable = true)\n",
      " |-- MAG_REF: string (nullable = true)\n",
      " |-- MAG_REF_ERR: string (nullable = true)\n",
      " |-- MASKFRAC: string (nullable = true)\n",
      " |-- MIN_DISTANCE_TO_EDGE_IN_NEW: string (nullable = true)\n",
      " |-- N2SIG3: string (nullable = true)\n",
      " |-- N2SIG3SHIFT: string (nullable = true)\n",
      " |-- N2SIG5: string (nullable = true)\n",
      " |-- N2SIG5SHIFT: string (nullable = true)\n",
      " |-- N3SIG3: string (nullable = true)\n",
      " |-- N3SIG3SHIFT: string (nullable = true)\n",
      " |-- N3SIG5: string (nullable = true)\n",
      " |-- N3SIG5SHIFT: string (nullable = true)\n",
      " |-- NN_DIST_RENORM: string (nullable = true)\n",
      " |-- NUMNEGRN: string (nullable = true)\n",
      " |-- SCALE: string (nullable = true)\n",
      " |-- SNR: string (nullable = true)\n",
      " |-- SPREADERR_MODEL: string (nullable = true)\n",
      " |-- SPREAD_MODEL: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "\n",
    "#describe a couple of the physics features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|OBJECT_TYPE| count|\n",
      "+-----------+------+\n",
      "|          0|454092|\n",
      "|          1|444871|\n",
      "+-----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('OBJECT_TYPE').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import Vectors, Vector, VectorUDT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the ML algo wants a tuple of label and a vector of the other features. Let's make a little function to convert rows to vectrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_row_to_vector(row, lbl_key='OBJECT_TYPE'):\n",
    "    row = row.asDict()\n",
    "    lbl = int(row[lbl_key])\n",
    "    float_list = [0.0 if str(v) == '' else float(v) for k,v in row.iteritems() if k!= lbl_key]\n",
    "    return (lbl, Vectors.dense(float_list))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we call map on the rdd in the dataframe, converting each row to a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lbl_vec_pairs = df.rdd.map(convert_row_to_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = spark.createDataFrame(lbl_vec_pairs, ['label', 'features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, IntegerType, StructType\n",
    "\n",
    "from pyspark.mllib.feature import LabeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data=lbl_vec_pairs.map(lambda (l,v): LabeledPoint(l,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "from pyspark.ml.feature import DecisionTreeParams \n",
    "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import TrainValidationSplitModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bce = BinaryClassificationEvaluator(metricName='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tr_data, te_data = data.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(rf.numTrees, [10, 50, 100, 300]) \\\n",
    "    .addGrid(rf.maxDepth, [100, 30, 15, 5]) \\\n",
    "    .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tvs = TrainValidationSplit(estimator=rf,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=bce,\n",
    "                           trainRatio=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tvs.fit(tr_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prediction = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook spark-astro-ml.ipynb to script\n",
      "[NbConvertApp] Writing 5728 bytes to spark-astro-ml.py\n"
     ]
    }
   ],
   "source": [
    "# convert to .py file. Now let's submit to queue\n",
    "! jupyter nbconvert --to script spark-astro-ml.ipynb\n",
    "!sed -i.bak '/ipython*/d' ./*.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HW!\n",
    "Items to Work on: 3 Options:\n",
    "\n",
    "1. ML\n",
    " * make a logistic regression model\n",
    " * use cross-validation to search a good space of logisitc regression hyoerparams\n",
    " * preprocess all features to mean zero and stdev 1\n",
    " * submit this job to batch\n",
    " \n",
    " \n",
    "2. Data Munging / Saving\n",
    " * find number of columns that have an element over 1\n",
    " * make a new data frame that contains \n",
    "     * the sum of GLUX SNR and GAUSS Columns\n",
    "     * a column with the max value from each row from the original data\n",
    "     * the mean value from each row\n",
    "     * the median\n",
    " * conver this data frame to pandas \n",
    " * also save this data frame out to JSON\n",
    "\n",
    " \n",
    "3. Deep Learning\n",
    "    * Train a convolutional neural network to classify the astronomy images for at least 50 epochs\n",
    "    * Submit this job to the quueue\n",
    "    * Plot the learning curve and an accuracy curve"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "pyspark (2.0.0)",
   "language": "python",
   "name": "pyspark_2.0.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
